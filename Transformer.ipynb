{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamza-Ali0237/PyTorch-Transformer-From-Scratch/blob/main/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7nw5GkJJ6nJ"
      },
      "source": [
        "# Encoder-Decoder Tranformer From Scratch Using PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implemeting The Encoder-Decoder Transformer Architecture From The 2017 Paper Published By Google [\"*Attention Is All You Need* \"](https://arxiv.org/abs/1706.03762)"
      ],
      "metadata": {
        "id": "GgdpC70oKARX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "S6oG30ERj6YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "pMlvhmCkJ6nO"
      },
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "import datasets\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embeddings = nn.Embedding(\n",
        "        vocab_size, d_model\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.embeddings(x) * math.sqrt(self.d_model)"
      ],
      "metadata": {
        "id": "B0p34cJvL7A7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, max_seq_len):\n",
        "    super().__init__()\n",
        "\n",
        "    pe = torch.zeros(max_seq_len, d_model)\n",
        "    position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0)/d_model))\n",
        "\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "Mjs1scoQxlXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super().__init__()\n",
        "\n",
        "    print(f\"Initializing MultiHeadAttention with d_model={d_model} and num_heads={num_heads}\")\n",
        "\n",
        "    assert d_model % num_heads == 0, 'd_model must be divisible by num_heads.'\n",
        "\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    self.head_dim = d_model // num_heads\n",
        "\n",
        "    self.query_linear = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.key_linear = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.value_linear = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    self.output_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "  def split_heads(self, x, batch_size):\n",
        "    seq_len = x.size(1)\n",
        "    x = x.reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "\n",
        "    return x.permute(0, 2, 1, 3)\n",
        "\n",
        "  def compute_attention(self, query, key, value, mask=None):\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "    if mask is not None:\n",
        "      scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "    return torch.matmul(attention_weights, value)\n",
        "\n",
        "  def combine_heads(self, x, batch_size):\n",
        "    x = x.permute(0, 2, 1, 3).contiguous()\n",
        "    return x.view(batch_size, -1, self.d_model)\n",
        "\n",
        "  def forward(self, q, k, v, mask=None):\n",
        "    batch_size = q.size(0)\n",
        "\n",
        "    query = self.split_heads(self.query_linear(q), batch_size)\n",
        "    key = self.split_heads(self.key_linear(k), batch_size)\n",
        "    value = self.split_heads(self.value_linear(v), batch_size)\n",
        "\n",
        "    attention_weights = self.compute_attention(query, key, value, mask)\n",
        "\n",
        "    output = self.combine_heads(attention_weights, batch_size)\n",
        "\n",
        "    return self.output_linear(output)"
      ],
      "metadata": {
        "id": "srg_Fyvsxk80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardSubLayer(nn.Module):\n",
        "  def __init__(self, d_model, d_ff):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(d_model, d_ff)\n",
        "    self.fc2 = nn.Linear(d_ff, d_model)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.fc2(self.relu(self.fc1(x)))"
      ],
      "metadata": {
        "id": "rgkPh9ewxk6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ff_sublayer = FeedForwardSubLayer(d_model, d_ff)\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, src_mask):\n",
        "    attn_output = self.self_attn(x, x, x, src_mask)\n",
        "\n",
        "    x = self.norm1(x + self.dropout(attn_output))\n",
        "\n",
        "    ff_output = self.ff_sublayer(x)\n",
        "\n",
        "    x = self.norm2(x + self.dropout(ff_output))\n",
        "\n",
        "    return x\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ff_sublayer = FeedForwardSubLayer(d_model, d_ff)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, tgt_mask, cross_mask):\n",
        "        # Self-attention\n",
        "        self_attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(self_attn_output))\n",
        "\n",
        "        # Cross-attention\n",
        "        cross_attn_output = self.cross_attn(x, encoder_output, encoder_output, cross_mask)\n",
        "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
        "\n",
        "        # Feed-forward\n",
        "        ff_output = self.ff_sublayer(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "B8sm3DBtxk1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_seq_length):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding = InputEmbeddings(vocab_size, d_model)\n",
        "\n",
        "    self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "    self.layers = nn.ModuleList([\n",
        "        EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
        "    ])\n",
        "\n",
        "  def forward(self, x, src_mask):\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    x = self.positional_encoding(x)\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, src_mask)\n",
        "\n",
        "    return x\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_seq_length):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = InputEmbeddings(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x, encoder_output, tgt_mask, cross_mask):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, tgt_mask, cross_mask)\n",
        "\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "PmtN97nOxkzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationHead(nn.Module):\n",
        "  def __init__(self, d_model, num_classes):\n",
        "    super().__init__()\n",
        "    self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    logits = self.fc(x)\n",
        "    return F.log_softmax(logits, dim=-1)"
      ],
      "metadata": {
        "id": "l6JeZo_Sxkwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = TransformerEncoder(vocab_size, d_model, num_heads, num_layers, d_ff, dropout, max_seq_len)\n",
        "        self.decoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_seq_len)\n",
        "\n",
        "    def forward(self, src, src_mask, tgt, tgt_mask):\n",
        "        encoder_output = self.encoder(src, src_mask)\n",
        "        decoder_output = self.decoder(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        return decoder_output"
      ],
      "metadata": {
        "id": "LN9DRpWjxkss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load \"WMT 2014 English-to-German\" Dataset\n",
        "dataset = load_dataset(\"wmt14\", \"de-en\")\n",
        "\n",
        "train_dataset = dataset['train']\n",
        "test_dataset = dataset['test']"
      ],
      "metadata": {
        "id": "fUBbaPAvAjjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset.features)"
      ],
      "metadata": {
        "id": "Y2eXQSNaniq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the data\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "def preprocess(batch):\n",
        "    src_texts = [example[\"en\"] for example in batch[\"translation\"]]\n",
        "    tgt_texts = [example[\"de\"] for example in batch[\"translation\"]]\n",
        "\n",
        "    src = tokenizer(\n",
        "        src_texts, padding=\"max_length\",\n",
        "        truncation=True, max_length=128, return_tensors=\"pt\"\n",
        "    )\n",
        "    tgt = tokenizer(\n",
        "        tgt_texts, padding=\"max_length\",\n",
        "        truncation=True, max_length=128, return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"src_input_ids\": src[\"input_ids\"].tolist(),\n",
        "        \"tgt_input_ids\": tgt[\"input_ids\"].tolist()\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "train_data = train_dataset.map(preprocess, batched=True, batch_size=64)"
      ],
      "metadata": {
        "id": "ekyqZ-sklGC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data[0])"
      ],
      "metadata": {
        "id": "1tqkjCvFpYdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define collate_fn to pad sequences dynamically\n",
        "def collate_fn(batch):\n",
        "    src_batch = [torch.tensor(item[\"src_input_ids\"]) for item in batch]\n",
        "    tgt_batch = [torch.tensor(item[\"tgt_input_ids\"]) for item in batch]\n",
        "\n",
        "    # Pad sequences in each batch\n",
        "    src_batch = torch.nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    tgt_batch = torch.nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "    return src_batch, tgt_batch\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=32, collate_fn=collate_fn, shuffle=True)"
      ],
      "metadata": {
        "id": "i7nz9lAMpjI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "0LjQc72MyzwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Hyperparameters\n",
        "vocab_size = tokenizer.vocab_size\n",
        "d_model = 504\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "max_seq_len = 128\n",
        "dropout = 0.1\n",
        "\n",
        "\n",
        "model = Transformer(vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "GCa_G-MUqADy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)"
      ],
      "metadata": {
        "id": "66bWHtrjrEpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to train the model\n",
        "def train(model, dataloader, criterion, optimizer, num_epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        for src_batch, tgt_batch in dataloader:\n",
        "            src_batch, tgt_batch = src_batch.to(\"cuda\"), tgt_batch.to(\"cuda\")\n",
        "\n",
        "            # Create masks\n",
        "            src_mask = (src_batch != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "            # Shift target for teacher forcing\n",
        "            tgt_input = tgt_batch[:, :-1]\n",
        "            tgt_output = tgt_batch[:, 1:]\n",
        "\n",
        "            # Create tgt_mask for tgt_input\n",
        "            tgt_mask = (tgt_input != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(src_batch, src_mask, tgt_input, tgt_mask)\n",
        "            loss = criterion(outputs.reshape(-1, vocab_size), tgt_output.reshape(-1))\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(dataloader):.4f}\")"
      ],
      "metadata": {
        "id": "yG1cbO0usuo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, train_dataloader, criterion, optimizer, num_epochs=10)"
      ],
      "metadata": {
        "id": "mafA2l3os3VB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-hslpAGftEJk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}